"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.promiseAllBatched = exports.execAndWaitAtLeast = exports.atomicQueue = exports.retry = exports.delay = void 0;
const logs_1 = require("@ledgerhq/logs");
const delay = (ms) => new Promise(f => setTimeout(f, ms));
exports.delay = delay;
const defaults = {
    maxRetry: 4,
    interval: 300,
    intervalMultiplicator: 1.5,
    context: "",
};
function retry(f, options) {
    const { maxRetry, interval, intervalMultiplicator, context } = Object.assign(Object.assign({}, defaults), options);
    function rec(remainingTry, i) {
        const result = f();
        if (remainingTry <= 0) {
            return result;
        }
        // In case of failure, wait the interval, retry the action
        return result.catch(e => {
            (0, logs_1.log)("promise-retry", context + " failed. " + remainingTry + " retry remain. " + String(e));
            return (0, exports.delay)(i).then(() => rec(remainingTry - 1, i * intervalMultiplicator));
        });
    }
    return rec(maxRetry, interval);
}
exports.retry = retry;
const atomicQueue = (job, queueIdentifier = () => "") => {
    const queues = {};
    return (...args) => {
        const id = queueIdentifier(...args);
        const queue = queues[id] || Promise.resolve();
        const p = queue.then(() => job(...args));
        queues[id] = p.catch(() => { });
        return p;
    };
};
exports.atomicQueue = atomicQueue;
function execAndWaitAtLeast(ms, cb) {
    const startTime = Date.now();
    return cb().then(r => {
        const remaining = ms - (Date.now() - startTime);
        if (remaining <= 0)
            return r;
        return (0, exports.delay)(remaining).then(() => r);
    });
}
exports.execAndWaitAtLeast = execAndWaitAtLeast;
/**
 * promiseAllBatched(n, items, i => f(i))
 * is essentially like
 * Promise.all(items.map(i => f(i)))
 * but with a guarantee that it will not create more than n concurrent call to f
 * where f is a function that returns a promise
 */
function promiseAllBatched(batch, items, fn) {
    return __awaiter(this, void 0, void 0, function* () {
        const data = Array(items.length);
        const queue = items.map((item, index) => ({
            item,
            index,
        }));
        function step() {
            return __awaiter(this, void 0, void 0, function* () {
                if (queue.length === 0)
                    return;
                const first = queue.shift();
                if (first) {
                    const { item, index } = first;
                    data[index] = yield fn(item, index);
                }
                yield step(); // each time an item redeem, we schedule another one
            });
        }
        // initially, we schedule <batch> items in parallel
        yield Promise.all(Array(Math.min(batch, items.length))
            .fill(() => undefined)
            .map(step));
        return data;
    });
}
exports.promiseAllBatched = promiseAllBatched;
//# sourceMappingURL=promise.js.map